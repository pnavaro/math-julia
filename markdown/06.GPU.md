```@meta
EditURL = "<unknown>/06.GPU.jl"
```

-*- coding: utf-8 -*-
---
jupyter:
  jupytext:
    cell_metadata_filter: -all
    formats: jl:light,ipynb
    text_representation:
      extension: .jl
      format_name: light
      format_version: '1.5'
      jupytext_version: 1.7.1
  kernelspec:
    display_name: Julia 1.5.3
    language: julia
    name: julia-1.5
---

# Installation

````@example 06.GPU
import Pkg; Pkg.activate(@__DIR__); Pkg.instantiate()

ENV["TMP"] = "/opt/app-root/src"

tempdir()

using BenchmarkTools
using CUDA
using Random
using Test
using LinearAlgebra
using ForwardDiff
using ProgressMeter
using Plots

CUDA.version()
````

Useful function to enable GPU version of your code

````@example 06.GPU
CUDA.functional()

for device in CUDA.devices()
    @show capability(device)
    @show name(device)
end
````

## Array programming

### Construction

````@example 06.GPU
a = CuArray{Float32,2}(undef, 2, 2)

similar(a)

a = CuArray([1,2,3])
````

## Transfer to CPU

b is allocated on the CPU, a data transfer is made

````@example 06.GPU
b = Array(a)

collect(a)
````

### API compatibilty with Base.Array

````@example 06.GPU
CUDA.ones(2)

a = CUDA.zeros(Float32, 2)

a isa AbstractArray

CUDA.fill(42, (3,4))
````

## Random numbers

````@example 06.GPU
CUDA.rand(2, 2)

CUDA.randn(2, 1)

x = CUDA.CuArray(0:0.01:1.0)
nt = length(x)
y = 0.2 .+ 0.5 .* x + 0.1 .* CUDA.randn(nt);
scatter( Array(x), Array(y))
plot!( x -> 0.2 + 0.5x)
xlims!(0,1)
ylims!(0,1)

X = hcat(CUDA.ones(nt), x);

β = X'X \ X'y

sum( ( β[1] .+ β[2] .* x .- y).^2 )

a = CuArray([1 2 3])

view(a, 2:3)
````

+

````@example 06.GPU
a = CuArray{Float64}([1 2 3])
b = CuArray{Float64}([4 5 6])

map(a) do x
    x + 1
end
````

-

````@example 06.GPU
reduce(+, a)


accumulate(+, b; dims=2)


findfirst(isequal(2), a)
````

+

````@example 06.GPU
a = CuArray([1 2 3])
b = CuArray([4 5 6])

map(a) do x
    x + 1
end

a .+ 2b

reduce(+, a)

accumulate(+, b; dims=2)

findfirst(isequal(2), a)
````

-

# CURAND

````@example 06.GPU
CUDA.rand!(a)
````

# CUBLAS

````@example 06.GPU
a * b'
````

# CUSOLVER

````@example 06.GPU
L, ipiv = CUDA.CUSOLVER.getrf!(a'b)
CUDA.CUSOLVER.getrs!('N', L, ipiv, CUDA.ones(Float64, 3))
````

# CUFFT

````@example 06.GPU
fft = CUFFT.plan_fft(a)
fft * a

ifft = CUFFT.plan_ifft(a)
real(ifft * (fft * a))
````

# CUDDN

````@example 06.GPU
CUDA.CUDNN.softmax(real(ans))
````

# CUSPARSE

````@example 06.GPU
CUDA.CUSPARSE.CuSparseMatrixCSR(a)
````

## Workflow

A typical approach for porting or developing an application for the GPU is as follows:

1. develop an application using generic array functionality, and test it on the CPU with the Array type
2. port your application to the GPU by switching to the CuArray type
3. disallow the CPU fallback ("scalar indexing") to find operations that are not implemented for or incompatible with GPU execution
4. (optional) use lower-level, CUDA-specific interfaces to implement missing functionality or optimize performance

## Linear regression example

squared error loss function

````@example 06.GPU
loss(w, b, x, y) = sum(abs2, y - (w*x .+ b)) / size(y, 2)
````

get gradient w.r.t to `w`

````@example 06.GPU
loss∇w(w, b, x, y) = ForwardDiff.gradient(w -> loss(w, b, x, y), w)
````

get derivative w.r.t to `b` (`ForwardDiff.derivative` is
used instead of `ForwardDiff.gradient` because `b` is
a scalar instead of an array)

````@example 06.GPU
lossdb(w, b, x, y) = ForwardDiff.derivative(b -> loss(w, b, x, y), b)
````

proximal gradient descent function

````@example 06.GPU
function train(w, b, x, y; lr=0.1)
    w -= lmul!(lr, loss∇w(w, b, x, y))
    b -= lr * lossdb(w, b, x, y)
    return w, b
end

function cpu_test(n = 1000, p = 100, iter = 100)
    x = randn(n, p)'
    y = sum(x[1:5,:]; dims=1) .+ randn(n)' * 0.1
    w = 0.0001 * randn(1, p)
    b = 0.0
    for i = 1:iter
       w, b = train(w, b, x, y)
    end
    return loss(w,b,x,y)
end

@time cpu_test()
````

### Moving to GPU

````@example 06.GPU
function gpu_test( n = 1000, p = 100, iter = 100)
    x = randn(n, p)'
    y = sum(x[1:5,:]; dims=1) .+ randn(n)' * 0.1
    w = 0.0001 * randn(1, p)
    b = 0.0
    x = CuArray(x)
    y = CuArray(y)
    w = CuArray(w)

    for i = 1:iter
       w, b = train(w, b, x, y)

    end
    return loss(w,b,x,y)
end

@time gpu_test()

@btime cpu_test( 10000, 100, 100)

@btime gpu_test( 10000, 100, 100);
nothing #hide
````

# Custom Kernel

- you cannot allocate memory,
- I/O is disallowed,
- badly-typed code will not compile.

Keep kernels simple, and only incrementally port code while continuously verifying that it still compiles and executes as expected.

+

````@example 06.GPU
a = CUDA.zeros(1024)

function kernel(a)
    i = threadIdx().x
    a[i] += 1
    return
end

@cuda threads=length(a) kernel(a)
````

-

````@example 06.GPU
a = CUDA.rand(Int, 1000)

norm(a)


@btime norm($a)


@btime norm($(Array(a)))
````

The `norm` computation is much faster on the CPU

````@example 06.GPU
CUDA.allowscalar(false)


a = CuArray(1:9_999_999);

@time a .+ reverse(a);
nothing #hide
````

You need two kernels to perfom this last computation. @time is not the right tool to evaluate the elasped time. The task is scheduled on the GPU device but not executed. It will be executed when you will fetch the result on the CPU.

````@example 06.GPU
@time CUDA.@sync a .+ reverse(a);

CUDA.@time a .+ reverse(a);

@btime CUDA.@sync $a .+ reverse($a);

@btime CUDA.@sync $(Array(a)) .+ reverse($(Array(a)));
nothing #hide
````

# NVIDIA Nsight Compute

```bash
$ nv-nsight-cu-cli --profile-from-start off julia
```
```julia-repl
julia> CUDAdrv.@profile a .+ reverse(a)
julia> exit()
```

```bash
$ nsys launch julia
```
```julia-repl
julia> CUDAdrv.@profile a .+ reverse(a)
```

# Interactive development

````@example 06.GPU
kernel() = (@cuprintln("foo"); return)

@cuda kernel()

kernel() = (@cuprintln("bar"); return)

@cuda kernel()
````

There is a significant overhead when you launch several kernels

````@example 06.GPU
a = CuArray(1:9_999_999)
c = similar(a)
c .= a .+ reverse(a);

function vadd_reverse(c, a, b)
    i = threadIdx().x
    if i <= length(c)
        @inbounds c[i] = a[i] + reverse(b)[i]
    end
    return
end
````

# Unsupported

```julia
@cuda threads = length(a) vadd_reverse(c, a, a)
```
will raise an error because some features are not supported on the GPU

- Dynamic allocations
- Garbage collection
- Dynamic dispatch
- Input/Output

````@example 06.GPU
function vadd_reverse(c, a, b)

    i = threadIdx().x
    if i <= length(c)
        @inbounds c[i] = a[i] + b[end - i + 1]
    end
    return
end
````

The following expression will also raise an error because you can't loop over an array
on GPU in the same way.
```julia
@cuda threads = length(a) vadd_reverse(c, a, a)
```

````@example 06.GPU
attribute(device(), CUDA.DEVICE_ATTRIBUTE_MAX_THREADS_PER_BLOCK)

function vadd_reverse(c, a, b)
    i = (blockIdx().x - 1) * blockDim().x + threadIdx().x
    if i <= length(c)
        @inbounds c[i] = a[i] + b[end - i + 1]
    end
    return
end
````

The kernel you built is twice faster

````@example 06.GPU
@btime CUDA.@sync @cuda threads=1024 blocks=length($a)÷1024+1 vadd_reverse($c, $a, $a)

@btime CUDA.@sync $a .+ reverse($a);
nothing #hide
````

To adapt the code to your device, use this configurator function

````@example 06.GPU
function configurator(kernel)
    config = launch_configuration(kernel.fun)
    threads = min(length(a), config.threads)
    blocks = cld(length(a), threads)
    return (threads=threads, blocks=blocks)
end

@cuda config=configurator vadd_reverse(c, a, a)
````

# 12 Performance optimization

- Avoid thread divergence
- Reduce and coalesce global accesses
- Improve occuancy, optimize launch configuration

- Early-free array using CUDA.unsafe_free!
- Annotate with @inbounds to avoid exception branches
- Use Int32 and floating-point values

````@example 06.GPU
@device_code_llvm @cuda vadd_reverse(c, a, a)
````

# Cooperative groups

https://devblogs.nvidia.com/cooperative-groups/

```julia
@cuda cooperative=true kernel(...)
```

# Shared memory

https://devblogs.nvidia.com/using-shared-memory-cuda-cc/

```julia
a = @cuStaticSharedMem(Int, 64)
```

# Dynamic parallelism

https://devblogs.nvidia.com/cuda-dynamic-parallelism-api-principles/

@cuda dynamic=true kernel(...)

# Standard output

```julia
@cuprintln("Thread $(threadIdx().x)")
```

# Atomics

```julia
@atomic a[...] += val
```

## GPU programming performance tips

- Avoid thread divergence (https://cvw.cac.cornell.edu/gpu/thread_div)
- Reduce and coalesce global accesses
- Improve occupancy
- Early-free arrays  `CuArrays.unsafe_free!` (https://juliagpu.gitlab.io/CUDA.jl/usage/memory/)
- Annotate with `@inbounds`
- Use 32 bits for float and integers

---

*This page was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*

