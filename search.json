[
  {
    "objectID": "01-put-code-inside-function.html",
    "href": "01-put-code-inside-function.html",
    "title": "Performance critical code should be inside a function",
    "section": "",
    "text": "Let’s compute \\(y = a * x\\)\n\nn = 100_000\na = 1.2\nx = rand(Float64, n)\ny = rand(Float64, n)\n\n@time for i in eachindex(y, x)\n    y[i] += a * x[i]\nend\n\n  0.041669 seconds (699.25 k allocations: 12.204 MiB, 45.32% compilation time)\n\n\n\nTo optimize the code, Julia needs it to be inside a function.\n\nfunction axpy!(y, a, x)\n    for i in eachindex(y, x)\n        y[i] += a * x[i]\n    end\nend\n\n# warmup\naxpy!(y, a, x)\n\n# timing\n@time axpy!(y, a, x)\n\n  0.000202 seconds"
  },
  {
    "objectID": "02-avoid-untyped-global-variable.html",
    "href": "02-avoid-untyped-global-variable.html",
    "title": "Avoid untyped global variables",
    "section": "",
    "text": "using BenchmarkTools\n\nvariable = 10 \n\nfunction add_using_global_variable(x)\n    return x + variable\nend\n\n@btime add_using_global_variable(10);\n\n[ Info: Precompiling BenchmarkTools [6e4b80f9-dd63-53aa-95a3-0cdb28fa8baf]\n\n\n  17.535 ns (0 allocations: 0 bytes)"
  },
  {
    "objectID": "02-avoid-untyped-global-variable.html#pass-the-variable-in-the-arguments-of-the-function",
    "href": "02-avoid-untyped-global-variable.html#pass-the-variable-in-the-arguments-of-the-function",
    "title": "Avoid untyped global variables",
    "section": "Pass the variable in the arguments of the function",
    "text": "Pass the variable in the arguments of the function\n\nfunction add_using_function_arg(x, y)\n    return x + y\nend\n\n@btime add_using_function_arg(10, $variable);\n\n  2.900 ns (0 allocations: 0 bytes)\n\n\n\n\n@code_llvm add_using_function_arg(10, variable)\n\n;  @ In[3]:1 within `add_using_function_arg`\ndefine \n\n\ni64 @julia_add_using_function_arg_1946(i64 signext %0, i64 signext %1) #0 {\ntop:\n;  @ In[3]:2 within `add_using_function_arg`\n; ┌ @ int.jl:87 within `+`\n   %2 = add i64 %1, %0\n; └\n  ret i64 %2\n}\n\n\n\n\n@code_llvm add_using_global_variable(10)\n\n;  @ In[2]:5 within `add_using_global_variable`\ndefine nonnull {}* @julia_add_using_global_variable_1969(i64 signext %0) #0 {\ntop:\n  %1 = alloca [2 x {}*], align 8\n  %gcframe2 = alloca [4 x {}*], align 16\n  %gcframe2.sub = getelementptr inbounds [4 x {}*], [4 x {}*]* %gcframe2, i64 0, i64 0\n  %.sub = getelementptr inbounds [2 x {}*], [2 x {}*]* %1, i64 0, i64 0\n  %2 = bitcast [4 x {}*]* %gcframe2 to i8*\n  call void @llvm.memset.p0i8.i32(i8* noundef nonnull align 16 dereferenceable(32) %2, i8 0, i32 32, i1 false)\n  %thread_ptr = call i8* asm \"movq %fs:0, $0\", \"=r\"() #3\n  %ppgcstack_i8 = getelementptr i8, i8* %thread_ptr, i64 -8\n  %ppgcstack = bitcast i8* %ppgcstack_i8 to {}****\n  %pgcstack = load {}***, {}**** %ppgcstack, align 8\n;  @ In[2]:6 within `add_using_global_variable`\n  %3 = bitcast [4 x {}*]* %gcframe2 to i64*\n  store i64 8, i64* %3, align 16\n  %4 = getelementptr inbounds [4 x {}*], [4 x {}*]* %gcframe2, i64 0, i64 1\n  %5 = bitcast {}** %4 to {}***\n  %6 = load {}**, {}*** %pgcstack, align 8\n  store {}** %6, {}*** %5, align 8\n  %7 = bitcast {}*** %pgcstack to {}***\n  store {}** %gcframe2.sub, {}*** %7, align 8\n  %8 = load atomic {}*, {}** inttoptr (i64 139888291249304 to {}**) unordered, align 8\n  %9 = getelementptr inbounds [4 x {}*], [4 x {}*]* %gcframe2, i64 0, i64 2\n  store {}* %8, {}** %9, align 16\n  %10 = call nonnull {}* @ijl_box_int64(i64 signext %0)\n  %11 = getelementptr inbounds [4 x {}*], [4 x {}*]* %gcframe2, i64 0, i64 3\n  store {}* %10, {}** %11, align 8\n  store {}* %10, {}** %.sub, align 8\n  %12 = getelementptr inbounds [2 x {}*], [2 x {}*]* %1, i64 0, i64 1\n  store {}* %8, {}** %12, align 8\n  %13 = call nonnull {}* @ijl_apply_generic({}* inttoptr (i64 139888081967520 to {}*), {}** nonnull %.sub, i32 2)\n  %14 = load {}*, {}** %4, align 8\n  %15 = bitcast {}*** %pgcstack to {}**\n  store {}* %14, {}** %15, align 8\n  ret {}* %13\n}"
  },
  {
    "objectID": "02-avoid-untyped-global-variable.html#set-type-of-the-global-variable",
    "href": "02-avoid-untyped-global-variable.html#set-type-of-the-global-variable",
    "title": "Avoid untyped global variables",
    "section": "Set type of the global variable",
    "text": "Set type of the global variable\n\nvariable_typed::Int = 10\n\nfunction add_using_global_variable_typed(x)\n    return x + variable_typed\nend\n\n@btime add_using_global_variable_typed(10);\n\n  2.500 ns (0 allocations: 0 bytes)"
  },
  {
    "objectID": "02-avoid-untyped-global-variable.html#use-the-keyword-const",
    "href": "02-avoid-untyped-global-variable.html#use-the-keyword-const",
    "title": "Avoid untyped global variables",
    "section": "Use the keyword const",
    "text": "Use the keyword const\n\nconst constant = 10\n\nfunction add_by_passing_global_constant(x, v)\n    return x + v\nend\n\n@btime add_by_passing_global_constant(10, $constant);\n\n  2.800 ns (0 allocations: 0 bytes)\n\n\n\n\nvariable = 10\n\nfunction sum_variable_many_times(n)\n    total = rand(variable)\n    for i in 1:n\n        total .+= rand(variable)\n    end\n    return total\nend\n\n@btime sum_variable_many_times(100);\n\n  73.801 μs (301 allocations: 20.45 KiB)\n\n\n\n\nconst constant = 10\n\nfunction sum_constant_many_times(n)\n    total = rand(constant)\n    for i in 1:n\n        total .+= rand(constant)\n    end\n    return total\nend\n\n@btime sum_constant_many_times(100);\n\n  12.900 μs (101 allocations: 14.20 KiB)"
  },
  {
    "objectID": "03-pay-attention-to-memory-allocation.html",
    "href": "03-pay-attention-to-memory-allocation.html",
    "title": "Pay attention to memory allocation",
    "section": "",
    "text": "function build_preallocate(n::Int)\n    @assert n >= 2\n    v = zeros(Int64,n)\n    v[1] = 1\n    v[2] = 1\n    for i = 3:n\n        v[i] = v[i-1] + v[i-2]\n    end\n    return v\nend\n\nbuild_preallocate (generic function with 1 method)\n\n\n\n\nfunction build_no_allocation(n::Int)\n    @assert n >= 2\n    v = Vector{Int64}()\n    push!(v,1)\n    push!(v,1)\n    for i = 3:n\n        push!(v,v[i-1]+v[i-2])\n    end\n    return v\nend\n\nbuild_no_allocation (generic function with 1 method)"
  },
  {
    "objectID": "03-pay-attention-to-memory-allocation.html#whenever-possible-preallocate-memory",
    "href": "03-pay-attention-to-memory-allocation.html#whenever-possible-preallocate-memory",
    "title": "Pay attention to memory allocation",
    "section": "Whenever possible, preallocate memory",
    "text": "Whenever possible, preallocate memory\n\nisequal(build_preallocate(10),build_no_allocation(10))\n\ntrue\n\n\n\nusing BenchmarkTools\n\nn = 100\n\n@btime build_no_allocation(n);\n\n@btime build_preallocate(n);\n\n  790.351 ns (4 allocations: 1.92 KiB)\n\n\n  152.556 ns (1 allocation: 896 bytes)\n\n\n\njulia --check-bounds=no -O3 --track-allocation=user build_no_allocation.jl\n\ncat build_no_allocation.jl.*.mem\n\n   - function build_no_allocation(n::Int)\n   0     @assert n >= 2\n  64     v = Vector{Int64}()\n  80     push!(v,1)\n   0     push!(v,1)\n   0     for i = 3:n\n1824         push!(v,v[i-1]+v[i-2])\n   0     end\n   0     return v\n   - end\n\njulia --check-bounds=no -O3 --track-allocation=user build_preallocate.jl\n\ncat build_preallocate.jl.*.mem\n\n  - function build_preallocate(n::Int)\n  0     @assert n >= 2\n896     v = zeros(Int64,n)\n  0     v[1] = 1\n  0     v[2] = 1\n  0     for i = 3:n\n  0         v[i] = v[i-1] + v[i-2]\n  0     end\n  0     return v\n  - end"
  },
  {
    "objectID": "04-avoid-containers-with-abstract-type.html",
    "href": "04-avoid-containers-with-abstract-type.html",
    "title": "Avoid containers with abstract type parameters",
    "section": "",
    "text": "a = Real[]\n\npush!(a, 1); push!(a, 2.0); push!(a, π)\n\n3-element Vector{Real}:\n 1\n 2.0\n π = 3.1415926535897...\n\n\nSince Real objects can be of arbitrary size and structure, a must be represented as an array of pointers to individually allocated Real objects. With concrete type Float64, b is stored as a contiguous block of 64-bit floating-point values that can be manipulated efficiently.\n\nb = Float64[]\n\npush!(b, 1); push!(b, 2.0); push!(b,  π)\n\n3-element Vector{Float64}:\n 1.0\n 2.0\n 3.141592653589793"
  },
  {
    "objectID": "05-avoid-struct-fields-with-abstract-type.html",
    "href": "05-avoid-struct-fields-with-abstract-type.html",
    "title": "Avoid struct fields with abstract type",
    "section": "",
    "text": "struct Cube\n    length\n    width\n    height\nend\n\nstruct CubeTyped\n    length::Float64\n    width::Float64\n    height::Float64\nend\n\nstruct CubeParametricTyped{T <: Real}\n    length::T\n    width::T\n    height::T\nend\n\n\n\nvolume(c) = c.length*c.width*c.height\n\nc1 = Cube(1.1,1.2,1.3)\nc2 = CubeTyped(1.1,1.2,1.3)\nc3 = CubeParametricTyped(1.1,1.2,1.3)\n@show volume(c1) == volume(c2) == volume(c3)\n\nvolume(c1) == volume(c2) == volume(c3) = true\n\n\ntrue\n\n\n\nusing BenchmarkTools\n@btime volume($c1) # not typed\n@btime volume($c2) # typed float\n@btime volume($c3) # typed parametric\n\n  20.361 ns (1 allocation: 16 bytes)\n\n\n  2.500 ns (0 allocations: 0 bytes)\n\n\n  2.500 ns (0 allocations: 0 bytes)\n\n\n1.7160000000000002\n\n\n\n\n@code_warntype volume(c1)\n\nMethodInstance for volume\n\n\n(::Cube)\n  from volume(c) in Main at In[3]:1\n\n\n\nArguments\n  #self#::Core.Const(volume)\n  c::Cube\nBody::Any\n\n\n1 ─ %1 = Base\n\n\n.getproperty(c, :length)\n\n\n::Any\n│   %2 = Base.getproperty(c, :width)::Any\n│   %3 = Base.getproperty(c, :height)::Any\n│   %4 = (%1 * %2 * %3)::Any\n└──      return %4\n\n\n\n\n\n@code_warntype volume(c2)\n\nMethodInstance for volume(::CubeTyped)\n  from volume(c) in Main at In[3]:1\nArguments\n  #self#::Core.Const(volume)\n  c::CubeTyped\nBody::Float64\n1 ─ %1 = Base.getproperty(c, :length)::Float64\n│   %2 = Base.getproperty(c, :width)::Float64\n│   %3 = Base.getproperty(c, :height)::Float64\n│   %4 = (%1 * %2 * %3)::Float64\n└──      return %4\n\n\n\n\n\n@code_warntype volume(c3)\n\nMethodInstance for volume(::CubeParametricTyped{Float64})\n  from volume(c) in Main at In[3]:1\nArguments\n  #self#::Core.Const(volume)\n  c::CubeParametricTyped{Float64}\nBody::Float64\n1 ─ %1 = Base.getproperty(c, :length)::Float64\n│   %2 = Base.getproperty(c, :width)::Float64\n│   %3 = Base.getproperty(c, :height)::Float64\n│   %4 = (%1 * %2 * %3)::Float64\n└──      return %4"
  },
  {
    "objectID": "06-break-functions-into-multiple-definitions.html",
    "href": "06-break-functions-into-multiple-definitions.html",
    "title": "Break functions into multiple definitions",
    "section": "",
    "text": "using LinearAlgebra\n\nfunction mynorm(A)\n    if isa(A, Vector)\n        return sqrt(real(dot(A,A)))\n    elseif isa(A, Matrix)\n        return maximum(svdvals(A))\n    else\n        error(\"mynorm: invalid argument\")\n    end\nend\nThis can be written more concisely and efficiently as:\nnorm(x::Vector) = sqrt(real(dot(x, x)))\n\nnorm(A::Matrix) = maximum(svdvals(A))"
  },
  {
    "objectID": "07-write-type-stable-functions.html",
    "href": "07-write-type-stable-functions.html",
    "title": "Write “type-stable” functions",
    "section": "",
    "text": "function square_plus_one(v::T) where T <:Number\n    g = v * v\n    return g + 1\nend\n\nsquare_plus_one (generic function with 1 method)\n\n\n\nv = rand()\n\n0.043261920548164357\n\n\n\n@code_warntype square_plus_one(v)\n\nMethodInstance for square_plus_one\n\n\n(::Float64)\n  from \n\n\nsquare_plus_one(v::T) where T<:Number\n\n\n in Main at In[2]:1\nStatic Parameters\n  T = Float64\nArguments\n  #self#::Core.Const(square_plus_one)\n  v::Float64\nLocals\n  g::Float64\nBody::Float64\n1 ─\n\n\n      \n\n\n(g = v * v)\n\n\n│   %2 = (g + 1)::Float64\n└──      return %2\n\n\n\n\n\nw = 5\n\n5\n\n\n\n@code_warntype square_plus_one(w)\n\nMethodInstance for square_plus_one(::Int64)\n  from square_plus_one(v::T) where T<:Number in Main at In[2]:1\nStatic Parameters\n  T = Int64\nArguments\n  #self#::Core.Const(square_plus_one)\n  v::Int64\nLocals\n  g::Int64\nBody::Int64\n1 ─      (g = v * v)\n│   %2 = (g + 1)::Int64\n└──      return %2\n\n\n\n\n\nGreat! In the above two examples, we were able to predict what the output will be. This is because:\n\nfunction square_plus_one(v::T) where T <:Number\n    g = v*v         # Type(T * T) ==> T\n    return g+1      # Type(T + Int)) ==> \"max\" (T,Int)\nend\n\n\nNote that in both calls the return type was different, once Float64 and once Int64. But the function is still type stable.\n\n\n\nfunction zero_or_val(x::Real)\n    if x >= 0\n        return x\n    else\n        return 0\n    end\nend\n@code_warntype zero_or_val(0.2)\n\nMethodInstance for zero_or_val(::Float64)\n  from zero_or_val(x::Real) in Main at In[7]:1\nArguments\n  #self#::Core.Const(zero_or_val)\n  x::Float64\nBody::Union{Float64, Int64}\n1 ─ %1 = (x >= 0)::Bool\n└──      goto #3 if not %1\n2 ─      return x\n3 ─      return 0\n\n\n\n\n\nYou can avoid type instable code by using the promote_type function which returns the highest of the two types passed.\n\n\nfunction zero_or_val_stable(x::Real)\n    if x >= 0\n        y = x\n    else\n        y = 0\n    end\n    T = promote_type(typeof(x),Int)\n    return T(y)\nend\n@code_warntype zero_or_val_stable(0.2)\n\nMethodInstance for zero_or_val_stable(::Float64)\n  from zero_or_val_stable(x::Real) in Main at In[8]:1\nArguments\n  #self#::Core.Const(zero_or_val_stable)\n  x::Float64\nLocals\n  T::Type{Float64}\n  y::Union{Float64, Int64}\nBody::Float64\n1 ─       \n\n\nCore.NewvarNode(:(T))\n│         Core.NewvarNode(:(y))\n│   %3  = (x >= 0)::Bool\n└──       goto #3 if not %3\n2 ─       (y = x)\n└──       goto #4\n3 ─       (y = 0)\n4 ┄ %8  = Main.typeof(x)::Core.Const(Float64)\n│         (T = Main.promote_type(%8, Main.Int))\n│   %10 = (T::\n\n\nCore.Const(Float64))(y)::Float64\n└──       return %10"
  },
  {
    "objectID": "08-avoid-changing-the-type-of-a-variable.html",
    "href": "08-avoid-changing-the-type-of-a-variable.html",
    "title": "Avoid changing the type of a variable",
    "section": "",
    "text": "Let us say we want to play the following game, I give you a vector of numbers. And you want to accumulate the sum as follows. For each number in the vector, you toss a coin (rand()), if it is heads (>=0.5), you add 1. Otherwise, you add the number itself.\n\nfunction flipcoin_then_add(v::Vector{T}) where T <: Real\n    s = 0\n    for vi in v\n        r = rand()\n        if r >=0.5\n            s += 1\n        else\n            s += vi\n        end\n    end\nend\n\nflipcoin_then_add (generic function with 1 method)\n\n\n\n\nfunction flipcoin_then_add_typed(v::Vector{T}) where T <: Real\n    s = zero(T)\n    for vi in v\n        r = rand()\n        if r >=0.5\n            s += one(T)\n        else\n            s += vi\n        end\n    end\nend\n\nflipcoin_then_add_typed (generic function with 1 method)\n\n\n\n\nusing BenchmarkTools\n\nmyvec = rand(1000)\n@show flipcoin_then_add(myvec) == flipcoin_then_add_typed(myvec)\n\nflipcoin_then_add(myvec) == flipcoin_then_add_typed(myvec) = true\n\n\ntrue\n\n\n\n@btime flipcoin_then_add(rand(1000))\n@btime flipcoin_then_add_typed(rand(1000))\n\n  8.934 μs (1 allocation: 7.94 KiB)\n\n\n  2.589 μs (1 allocation: 7.94 KiB)"
  },
  {
    "objectID": "09-access-arrays-in-memory-order.html",
    "href": "09-access-arrays-in-memory-order.html",
    "title": "Access arrays in memory order, along columns",
    "section": "",
    "text": "function compute_dist!(x, dist)\n    for i=eachindex(x)\n        for j=eachindex(x)\n            dist[i, j] = abs(x[i] - x[j])\n        end\n    end\nend\n\nN = 10_000\nx = rand(Float64, N)\ndist = Array{Float64}(undef, (N, N))\n\ncompute_dist!(x, dist)\n@time compute_dist!(x, dist)\n\n  0.587318 seconds\n\n\n\n\nfunction compute_dist!(x, dist)\n    for j=eachindex(x)\n        for i=eachindex(x)\n            dist[i, j] = abs(x[i] - x[j])\n        end\n    end\nend\n\nN = 10_000\nx = rand(Float64, N)\ndist = Array{Float64}(undef, (N, N))\n\ncompute_dist!(x, dist)\n@time compute_dist!(x, dist)\n\n  0.138186 seconds\n\n\n\n\nusing BenchmarkTools, FFTW\nxmin, xmax, nx = 0, 4π, 1024\nymin, ymax, ny = 0, 4π, 1024\nx = LinRange(xmin, xmax, nx+1)[1:end-1]\ny = LinRange(ymin, ymax, ny+1)[1:end-1]\n\nfunction df_dy!( f )\n    ky  = 2π ./ (ymax-ymin) .* fftfreq(ny, ny)\n    exky = exp.( 1im .* ky' .* x)\n    f .= real(ifft(exky .* fft(f, 2), 2))\nend\n\nf1 = sin.(x) .* cos.(y') \ndf_dy!( f1 )\n\n[ Info: Precompiling FFTW [7a1cc6ca-52ef-59f5-83cd-3a7055c09341]\n\n\n1024×1024 Matrix{Float64}:\n  0.0         0.0         0.0        …   0.0         0.0         0.0\n  0.0122706   0.0122678   0.0122632      0.0122678   0.0122706   0.0122715\n  0.0245338   0.0245246   0.0245117      0.0245394   0.0245412   0.0245394\n  0.0367823   0.0367629   0.036738       0.0368072   0.0368045   0.0367961\n  0.0490086   0.0489753   0.0489347      0.049064    0.0490529   0.0490344\n  0.0612053   0.0611546   0.0610946  …   0.0613023   0.0612792   0.0612469\n  0.0733652   0.0732933   0.0732103      0.0735147   0.073476    0.0734261\n  0.0854809   0.0853842   0.0852745      0.085694    0.0856359   0.0855648\n  0.0975452   0.0974199   0.09728        0.0978327   0.0977516   0.0976557\n  0.109551    0.109393    0.109219       0.109924    0.109816    0.109691\n  0.12149     0.121297    0.121086   …   0.121959    0.121821    0.121665\n  0.133356    0.133124    0.132872       0.133933    0.133761    0.133569\n  0.145142    0.144867    0.14457        0.145836    0.145627    0.145396\n  ⋮                                  ⋱                          \n -0.145142   -0.145396   -0.145627      -0.144252   -0.14457    -0.144867\n -0.133356   -0.133569   -0.133761      -0.132599   -0.132872   -0.133124\n -0.12149    -0.121665   -0.121821      -0.120856   -0.121086   -0.121297\n -0.109551   -0.109691   -0.109816   …  -0.109029   -0.109219   -0.109393\n -0.0975452  -0.0976557  -0.0977516     -0.0971254  -0.09728    -0.0974199\n -0.0854809  -0.0855648  -0.0856359     -0.0851521  -0.0852745  -0.0853842\n -0.0733652  -0.0734261  -0.073476      -0.0731163  -0.0732103  -0.0732933\n -0.0612053  -0.0612469  -0.0612792     -0.0610255  -0.0610946  -0.0611546\n -0.0490086  -0.0490344  -0.0490529  …  -0.0488867  -0.0489347  -0.0489753\n -0.0367823  -0.0367961  -0.0368045     -0.0367075  -0.036738   -0.0367629\n -0.0245338  -0.0245394  -0.0245412     -0.024495   -0.0245117  -0.0245246\n -0.0122706  -0.0122715  -0.0122706     -0.0122568  -0.0122632  -0.0122678\n\n\n\n\nfunction df_dy_transposed!( f )\n    ft = transpose(f)\n    ky  = 2π ./ (ymax-ymin) .* fftfreq(ny, ny)\n    exky = exp.( 1im .* ky .* x')\n    f .= transpose(real(ifft(exky .* fft(ft, 1), 1)))\nend\nf2 = sin.(x) .* cos.(y') \ndf_dy_transposed!( f2 )\n\n1024×1024 Matrix{Float64}:\n  0.0         0.0         0.0        …   0.0         0.0         0.0\n  0.0122706   0.0122678   0.0122632      0.0122678   0.0122706   0.0122715\n  0.0245338   0.0245246   0.0245117      0.0245394   0.0245412   0.0245394\n  0.0367823   0.0367629   0.036738       0.0368072   0.0368045   0.0367961\n  0.0490086   0.0489753   0.0489347      0.049064    0.0490529   0.0490344\n  0.0612053   0.0611546   0.0610946  …   0.0613023   0.0612792   0.0612469\n  0.0733652   0.0732933   0.0732103      0.0735147   0.073476    0.0734261\n  0.0854809   0.0853842   0.0852745      0.085694    0.0856359   0.0855648\n  0.0975452   0.0974199   0.09728        0.0978327   0.0977516   0.0976557\n  0.109551    0.109393    0.109219       0.109924    0.109816    0.109691\n  0.12149     0.121297    0.121086   …   0.121959    0.121821    0.121665\n  0.133356    0.133124    0.132872       0.133933    0.133761    0.133569\n  0.145142    0.144867    0.14457        0.145836    0.145627    0.145396\n  ⋮                                  ⋱                          \n -0.145142   -0.145396   -0.145627      -0.144252   -0.14457    -0.144867\n -0.133356   -0.133569   -0.133761      -0.132599   -0.132872   -0.133124\n -0.12149    -0.121665   -0.121821      -0.120856   -0.121086   -0.121297\n -0.109551   -0.109691   -0.109816   …  -0.109029   -0.109219   -0.109393\n -0.0975452  -0.0976557  -0.0977516     -0.0971254  -0.09728    -0.0974199\n -0.0854809  -0.0855648  -0.0856359     -0.0851521  -0.0852745  -0.0853842\n -0.0733652  -0.0734261  -0.073476      -0.0731163  -0.0732103  -0.0732933\n -0.0612053  -0.0612469  -0.0612792     -0.0610255  -0.0610946  -0.0611546\n -0.0490086  -0.0490344  -0.0490529  …  -0.0488867  -0.0489347  -0.0489753\n -0.0367823  -0.0367961  -0.0368045     -0.0367075  -0.036738   -0.0367629\n -0.0245338  -0.0245394  -0.0245412     -0.024495   -0.0245117  -0.0245246\n -0.0122706  -0.0122715  -0.0122706     -0.0122568  -0.0122632  -0.0122678\n\n\n\n\nisequal(f1, f2)\n\ntrue\n\n\n\nf = sin.(x) .* cos.(y')\n@btime df_dy!($f);\nf = sin.(x) .* cos.(y')\n@btime df_dy_transposed!($f);\n\n  85.934 ms (77 allocations: 88.01 MiB)\n\n\n  60.787 ms (78 allocations: 88.01 MiB)"
  },
  {
    "objectID": "10-preallocating-outputs.html",
    "href": "10-preallocating-outputs.html",
    "title": "Pre-allocating outputs",
    "section": "",
    "text": "You have a vector b and a vector h where b[i] is the base length of triangle i and h[i] is the height length. The experiment is to find the hypotenuse value of all triangles.\n\n\nusing BenchmarkTools\n\nb = rand(1000)*10\nh = rand(1000)*10\nfunction find_hypotenuse(b::Vector{T},h::Vector{T}) where T <: Real\n    return sqrt.(b.^2+h.^2)\nend\n\nfind_hypotenuse (generic function with 1 method)\n\n\n\n@btime find_hypotenuse($b, $h);\n\n  6.325 μs (4 allocations: 31.75 KiB)\n\n\n\n\nfunction find_hypotenuse_optimized(b::Vector{T},h::Vector{T}) where T <: Real\n    accum_vec = similar(b)\n    for i = eachindex(accum_vec)\n        accum_vec[i] = b[i]^2\n        accum_vec[i] += h[i]^2 # here, we used the same space in memory to hold the sum\n        accum_vec[i] = sqrt(accum_vec[i]) # same thing here, to hold the sqrt\n    end\n    return accum_vec\nend\n\nfind_hypotenuse_optimized (generic function with 1 method)\n\n\n\n@btime find_hypotenuse_optimized($b, $h);\n\n  2.333 μs (1 allocation: 7.94 KiB)\n\n\n\n\nusing FFTW, LinearAlgebra\n\nxmin, xmax, nx = 0, 4π, 1024\nymin, ymax, ny = 0, 4π, 1024\n\nx = LinRange(xmin, xmax, nx+1)[1:end-1]\ny = LinRange(ymin, ymax, ny+1)[1:end-1]\nky  = 2π ./ (ymax-ymin) .* fftfreq(ny, ny)\nexky = exp.( 1im .* ky .* x')\n\nf  = zeros(ComplexF64, (nx,ny))\nfᵗ = zeros(ComplexF64, reverse(size(f)))\nf̂ᵗ = zeros(ComplexF64, reverse(size(f)))\nf .= sin.(x) .* cos.(y')\n\nplan = plan_fft(fᵗ, 1, flags=FFTW.PATIENT)\n\nFFTW forward plan for 1024×1024 array of ComplexF64\n(dft-vrank>=1-x1024/1\n  (dft-ct-dit/32\n    (dftw-direct-32/16 \"t3fv_32_avx2\")\n    (dft-direct-32-x32 \"n2fv_32_avx2\")))\n\n\n\n\nfunction df_dy_optimized!( f, fᵗ, f̂ᵗ, plan, exky )\n\n    transpose!(fᵗ,f)\n    mul!(f̂ᵗ,  plan, fᵗ)\n    f̂ᵗ .= f̂ᵗ .* exky\n    ldiv!(fᵗ, plan, f̂ᵗ)\n    transpose!(f, fᵗ)\n\nend\n\n@btime df_dy_optimized!($f, $fᵗ, $f̂ᵗ, $plan, $exky );\n\n  26.159 ms (2 allocations: 112 bytes)"
  },
  {
    "objectID": "11-fused-vectorized-operations.html",
    "href": "11-fused-vectorized-operations.html",
    "title": "Fuse vectorized operations",
    "section": "",
    "text": "f(x) = 3x.^2 + 4x + 7x.^3;\n\nfdot(x) = @. 3x^2 + 4x + 7x^3; # = 3 .* x.^2 .+ 4 .* x .+ 7 .* x.^3\n\nBoth f and fdot compute the same thing.\n\nx = rand(10^6);\nf(x) # warmup\n@time f(x);\n\n  0.024205 seconds (12 allocations: 45.777 MiB)\n\n\n\nfdot(x) # warmup\n@time fdot(x);\n\n  0.003904 seconds (2 allocations: 7.629 MiB)\n\n\n\nf.(x) # warmup\n@time f.(x);\n\n  0.004554 seconds (4 allocations: 7.629 MiB)\n\n\n\nfdot(x) is faster and allocates less memory, because each * and + operation in f(x) allocates a new temporary array and executes in a separate loop."
  },
  {
    "objectID": "12-consider-using-views-for-slices.html",
    "href": "12-consider-using-views-for-slices.html",
    "title": "Consider using views for slices",
    "section": "",
    "text": "const N = 50_000_000\nconst a = 1.2\nconst x = rand(Float64, N)\nconst y = rand(Float64, N)\n\nconst nn = 100\nconst n_start = 1 + nn\nconst n_end = N - nn\n\n# warmup\n@. y[n_start:n_end] += a * x[n_start:n_end]\n\n# timing\n@time @. y[n_start:n_end] += a * x[n_start:n_end];\n\n  0.899913 seconds (4 allocations: 762.936 MiB, 21.02% gc time)\n\n\n\n# warmup\n@. @views y[n_start:n_end] += a * x[n_start:n_end]\n\n# timing\n@time @. @views y[n_start:n_end] += a * x[n_start:n_end];\n\n  0.337498 seconds"
  },
  {
    "objectID": "13-copy-irregularly-accessed-data.html",
    "href": "13-copy-irregularly-accessed-data.html",
    "title": "Copy irregularly-accessed data into a contiguous array before operating on it",
    "section": "",
    "text": "using Random\n\nx = randn(1_000_000);\n\ninds = shuffle(1:1_000_000)[1:800000];\n\nA = randn(50, 1_000_000);\n\nxtmp = zeros(800_000);\n\nAtmp = zeros(50, 800_000);\n\n@time sum(view(A, :, inds) * view(x, inds))\n@time sum(view(A, :, inds) * view(x, inds))\n\n  0.533371 seconds (402.54 k allocations: 19.418 MiB, 40.09% compilation time)\n\n\n  0.338892 seconds (5 allocations: 624 bytes)\n\n\n-10530.38283821043\n\n\n\nIrregular access patterns and non-contiguous views can drastically slow down computations on arrays because of non-sequential memory access. Copying the views into plain arrays speeds up the multiplication even with the cost of the copying operation.\n\n\n\n@time begin\n    copyto!(xtmp, view(x, inds))\n    copyto!(Atmp, view(A, :, inds))\n    sum(Atmp * xtmp)\nend\n\n  0.521841 seconds (375.71 k allocations: 19.544 MiB, 37.29% compilation time)\n\n\n-10530.382838210473\n\n\n\n@time begin\n    copyto!(xtmp, view(x, inds))\n    copyto!(Atmp, view(A, :, inds))\n    sum(Atmp * xtmp)\nend\n\n  0.303771 seconds (5 allocations: 624 bytes)\n\n\n-10530.382838210473"
  },
  {
    "objectID": "14-consider-static-arrays.html",
    "href": "14-consider-static-arrays.html",
    "title": "Consider StaticArrays.jl for small fixed-size vector/matrix operations",
    "section": "",
    "text": "using DifferentialEquations, BenchmarkTools\n\n[ Info: Precompiling DifferentialEquations [0c46a032-eb83-5123-abaf-570d42b7fbaa]\n\n\n┌ Warning: Replacing docs for `SciMLOperators.AbstractSciMLOperator :: Union{}` in module `SciMLOperators`\n└ @ Base.Docs docs/Docs.jl:240\n\n\n┌ Warning: Replacing docs for `SciMLBase.sol :: Union{Tuple, Tuple{D}, Tuple{S}, Tuple{N}, Tuple{T}} where {T, N, S, D}` in module `SciMLBase`\n└ @ Base.Docs docs/Docs.jl:240\n\n\n\n\nfunction lorenz(u,p,t)\n dx = 10.0*(u[2]-u[1])\n dy = u[1]*(28.0-u[3]) - u[2]\n dz = u[1]*u[2] - (8/3)*u[3]\n [dx,dy,dz]\nend\n\nlorenz (generic function with 1 method)\n\n\n\nu0 = [1.0;0.0;0.0]\ntspan = (0.0,100.0)\nprob = ODEProblem(lorenz,u0,tspan)\n@benchmark solve(prob,Tsit5())\n\n\nBenchmarkTools.Trial: 550 samples with 1 evaluation.\n Range (min … max):  3.740 ms … 53.190 ms  ┊ GC (min … max):  0.00% … 86.10%\n Time  (median):     8.530 ms              ┊ GC (median):     0.00%\n Time  (mean ± σ):   9.090 ms ±  8.431 ms  ┊ GC (mean ± σ):  18.28% ± 16.63%\n  ▆▆▄  ▆█▅                                                    \n  ████▇███▇▅▁▁▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▅▅▅▄▄ ▇\n  3.74 ms      Histogram: log(frequency) by time     51.2 ms <\n Memory estimate: 7.82 MiB, allocs estimate: 101102.\n\n\n\n\n\nfunction lorenz!(du,u,p,t)\n du[1] = 10.0*(u[2]-u[1])\n du[2] = u[1]*(28.0-u[3]) - u[2]\n du[3] = u[1]*u[2] - (8/3)*u[3]\nend\n\nlorenz! (generic function with 1 method)\n\n\n\nu0 = [1.0;0.0;0.0]\ntspan = (0.0,100.0)\nprob = ODEProblem(lorenz!,u0,tspan)\n@benchmark solve(prob,Tsit5())\n\n\nBenchmarkTools.Trial: 3507 samples with 1 evaluation.\n Range (min … max):  751.605 μs … 44.631 ms  ┊ GC (min … max):  0.00% … 95.46%\n Time  (median):       1.263 ms              ┊ GC (median):     0.00%\n Time  (mean ± σ):     1.419 ms ±  2.852 ms  ┊ GC (mean ± σ):  13.69% ±  6.64%\n  ▁▄▅▇▅▄▄▄▁         ▃▅█▇▄█▅▅▃▂▂▁                                \n  █████████▇█▇▇▆▆▅▅▅█████████████▇▆▇▆▄▅▃▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▂ ▅\n  752 μs          Histogram: frequency by time         2.28 ms <\n Memory estimate: 996.33 KiB, allocs estimate: 11415.\n\n\n\n\n\nStaticArray is statically-sized (known at compile time) and thus its accesses are quick. Additionally, the exact block of memory is known in advance by the compiler, and thus re-using the memory is cheap. This means that allocating on the stack has essentially no cost!\n\n\nusing StaticArrays\n\nfunction lorenz_static(u,p,t)\n dx = 10.0*(u[2]-u[1])\n dy = u[1]*(28.0-u[3]) - u[2]\n dz = u[1]*u[2] - (8/3)*u[3]\n @SVector [dx,dy,dz]\nend\n\nlorenz_static (generic function with 1 method)\n\n\n\nu0 = @SVector [1.0,0.0,0.0]\ntspan = (0.0,100.0)\nprob = ODEProblem(lorenz_static,u0,tspan)\n@benchmark solve(prob,Tsit5())\n\n\nBenchmarkTools.Trial: 7376 samples with 1 evaluation.\n Range (min … max):  337.704 μs … 35.141 ms  ┊ GC (min … max): 0.00% … 96.19%\n Time  (median):     612.056 μs              ┊ GC (median):    0.00%\n Time  (mean ± σ):   668.379 μs ±  1.277 ms  ┊ GC (mean ± σ):  7.86% ±  4.05%\n                ▃▃█▄▆▅▄▂▁▁▂▄▃▂▄▅▇▅▅▅▂▂▁▁                        \n  ▁▁▂▂▄▃▂▃▃▃▆▅▆██████████████████████████▇▅▆▄▄▄▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁ ▄\n  338 μs          Histogram: frequency by time          957 μs <\n Memory estimate: 387.39 KiB, allocs estimate: 1293."
  },
  {
    "objectID": "15-avoid-string-interpolation-for-io.html",
    "href": "15-avoid-string-interpolation-for-io.html",
    "title": "Avoid string interpolation for I/O",
    "section": "",
    "text": "When writing data to a file (or other I/O device), forming extra intermediate strings is a source of overhead. Instead of:\n\nprintln(file, \"$a $b\")\nuse:\nprintln(file, a, \" \", b)"
  },
  {
    "objectID": "16-performance-annotations.html",
    "href": "16-performance-annotations.html",
    "title": "Performance Annotations: @fastmath @inbounds @simd",
    "section": "",
    "text": "function new_sum(myvec::Vector{Int})\n    s = zero(eltype(myvec))\n    for i = eachindex(myvec)\n        s += myvec[i]\n    end\n    return s\nend\n\nfunction new_sum_inbounds(myvec::Vector{Int})\n    s = zero(eltype(myvec))\n    @inbounds for i = eachindex(myvec)\n        s += myvec[i]\n    end\n    return s\nend\n\nnew_sum_inbounds (generic function with 1 method)\n\n\n\nusing BenchmarkTools\n\nmyvec = collect(1:1000000)\n@btime new_sum($myvec)\n@btime new_sum_inbounds($myvec)\n\n  212.401 μs (0 allocations: 0 bytes)\n\n\n  212.601 μs (0 allocations: 0 bytes)\n\n\n500000500000\n\n\n\n\n\n\n@noinline function inner(x, y)\n    s = zero(eltype(x))\n    for i = eachindex(x, y)\n        @inbounds s += x[i]*y[i]\n    end\n    return s\nend;\n\n\n\n@noinline function innersimd(x, y)\n    s = zero(eltype(x))\n    @simd for i = eachindex(x, y)\n        @inbounds s += x[i] * y[i]\n    end\n    return s\nend;\n\n\n\n\n\nfunction timeit(n, reps)\n    x = rand(Float32, n)\n    y = rand(Float32, n)\n    s = zero(Float64)\n    time = @elapsed for j in 1:reps\n        s += inner(x, y)\n    end\n    println(\"GFlop/sec        = \", 2n*reps / time*1E-9)\n    time = @elapsed for j in 1:reps\n        s += innersimd(x, y)\n    end\n    println(\"GFlop/sec (SIMD) = \", 2n*reps / time*1E-9)\nend\ntimeit(10, 10)\ntimeit(1000, 1000)\n\nGFlop/sec        = 1.0\nGFlop/sec (SIMD) = 0.22222222222222227\nGFlop/sec        = 1.612894120758996\nGFlop/sec (SIMD) = 24.783147459727388\n\n\n\n\nfunction init!(u::Vector)\n    n = length(u)\n    dx = 1.0 / (n-1)\n    @fastmath @inbounds @simd for i in eachindex(u) \n        u[i] = sin(2pi*dx*i)\n    end\nend\n\nfunction deriv!(u::Vector, du)\n    n = length(u)\n    dx = 1.0 / (n-1)\n    @fastmath @inbounds du[1] = (u[2] - u[1]) / dx\n    @fastmath @inbounds @simd for i in 2:n-1\n        du[i] = (u[i+1] - u[i-1]) / (2*dx)\n    end\n    @fastmath @inbounds du[n] = (u[n] - u[n-1]) / dx\nend\n\nderiv! (generic function with 1 method)\n\n\n\n\nfunction mynorm(u::Vector)\n    T = eltype(u)\n    s = zero(T)\n    @fastmath @inbounds @simd for i in eachindex(u)\n        s += u[i]^2\n    end\n    @fastmath @inbounds return sqrt(s)\nend\n\nmynorm (generic function with 1 method)\n\n\n\n\nfunction main(n)\n    u = Vector{Float64}(undef, n)\n    init!(u)\n    du = similar(u)\n\n    deriv!(u, du)\n    nu = mynorm(du)\n\n    @time for i in 1:10^6\n        deriv!(u, du)\n        nu = mynorm(du)\n    end\n\n    println(\" nu = $nu \")\nend\n\nmain(10)\n@time main(2000)\n\n  0.026809 seconds\n nu = 13.187330309540112 \n\n\n  0.863798 seconds\n nu = 198.74110382490193 \n  0.864165 seconds (167 allocations: 42.742 KiB)\n\n\n\n\nrun(`julia --math-mode=ieee wave.jl`)\n\n  0.052124 seconds\n nu = 13.187330309540112 \n\n\n  3.801187 seconds\n nu = 198.74110382490198 \n  3.801334 seconds (25 allocations: 33.055 KiB)\n\n\n\nProcess(`julia --math-mode=ieee wave.jl`, ProcessExited(0))"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Performance examples in Julia",
    "section": "",
    "text": "Performance critical code should be inside a function\nAvoid untyped global variables\nPay attention to memory allocation\nAvoid containers with abstract type parameters\nAvoid struct fields with abstract type\nBreak functions into multiple definitions\nWrite “type-stable” functions\nAvoid changing the type of a variable\nAccess arrays in memory order, along columns\nPre-allocating outputs\nFuse vectorized operations\nConsider using views for slices\nCopy irregularly-accessed data into a contiguous array before operating on it\nConsider StaticArrays.jl for small fixed-size vector/matrix operations\nAvoid string interpolation for I/O\nPerformance Annotations: @fastmath @inbounds @simd"
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "Performance examples in Julia",
    "section": "References",
    "text": "References\n\nJulia Docs https://docs.julialang.org/en/v1/manual/performance-tips/\nBenoît Fabrèges https://plmlab.math.cnrs.fr/fabreges/julia-2019/\nNassar Huda https://github.com/nassarhuda/JuliaTutorials\nTom Kwong https://github.com/PacktPublishing/Hands-on-Design-Patterns-and-Best-Practices-with-Julia/"
  }
]