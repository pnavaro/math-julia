{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "-*- coding: utf-8 -*-\n",
    "---\n",
    "jupyter:\n",
    "  jupytext:\n",
    "    cell_metadata_filter: -all\n",
    "    formats: jl:light,ipynb\n",
    "    text_representation:\n",
    "      extension: .jl\n",
    "      format_name: light\n",
    "      format_version: '1.5'\n",
    "      jupytext_version: 1.6.0\n",
    "  kernelspec:\n",
    "    display_name: Julia 1.5.2\n",
    "    language: julia\n",
    "    name: julia-1.5\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Installation"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "import Pkg; Pkg.activate(@__DIR__); Pkg.instantiate()\n",
    "\n",
    "using Plots\n",
    "\n",
    "using BenchmarkTools\n",
    "using CUDA\n",
    "using Random\n",
    "using Test\n",
    "using LinearAlgebra\n",
    "using ForwardDiff\n",
    "using ProgressMeter\n",
    "\n",
    "CUDA.version()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Useful function to enable GPU version of your code"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "CUDA.functional()\n",
    "\n",
    "for device in CUDA.devices()\n",
    "    @show capability(device)\n",
    "    @show name(device)\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Array programming"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Construction"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "a = CuArray{Float32,2}(undef, 2, 2)\n",
    "\n",
    "similar(a)\n",
    "\n",
    "a = CuArray([1,2,3])"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Transfer to CPU\n",
    "\n",
    "b is allocated on the CPU, a data transfer is made"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "b = Array(a)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### API compatibilty with Base.Array"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "CUDA.ones(2)\n",
    "\n",
    "a = CUDA.zeros(Float32, 2)\n",
    "\n",
    "a isa AbstractArray\n",
    "\n",
    "CUDA.fill(42, (3,4))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Random numbers"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "CUDA.rand(2, 2)\n",
    "\n",
    "CUDA.randn(2, 1)\n",
    "\n",
    "x = CUDA.CuArray(0:0.01:1.0)\n",
    "nt = length(x)\n",
    "y = 0.2 .+ 0.5 .* x + 0.1 .* CUDA.randn(nt);\n",
    "scatter( Array(x), Array(y))\n",
    "plot!( x -> 0.2 + 0.5x)\n",
    "xlims!(0,1)\n",
    "ylims!(0,1)\n",
    "\n",
    "X = hcat(CUDA.ones(nt), x)\n",
    "\n",
    "@show β = pinv(X)  * y\n",
    "\n",
    "a = CuArray([1 2 3])\n",
    "\n",
    "view(a, 2:3)\n",
    "\n",
    "sum(a)\n",
    "\n",
    "a * 2\n",
    "\n",
    "a'\n",
    "\n",
    "a * CuArray([1, 2, 3])\n",
    "\n",
    "a = CuArray{Float32}(undef, (2,2))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "+"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "a = CuArray([1 2 3])\n",
    "b = CuArray([4 5 6])\n",
    "\n",
    "map(a) do x\n",
    "    x + 1\n",
    "end\n",
    "\n",
    "a .+ 2b\n",
    "\n",
    "reduce(+, a)\n",
    "\n",
    "accumulate(+, b; dims=2)\n",
    "\n",
    "findfirst(isequal(2), a)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "-"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CURAND"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "CUDA.rand!(a)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CUBLAS"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "a * a"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CUSOLVER"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using LinearAlgebra\n",
    "LinearAlgebra.qr!(a)\n",
    "\n",
    "L, ipiv = CUDA.CUSOLVER.getrf!(a)\n",
    "CUDA.CUSOLVER.getrs!('N', L, ipiv, CUDA.ones(2))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CUFFT"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "fft = CUFFT.plan_fft(a)\n",
    "fft * a\n",
    "\n",
    "ifft = CUFFT.plan_ifft(a)\n",
    "real(ifft * (fft * a))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CUDDN"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "CUDA.CUDNN.softmax(real(ans))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CUSPARSE"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "CUDA.CUSPARSE.CuSparseMatrixCSR(a)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Workflow\n",
    "\n",
    "A typical approach for porting or developing an application for the GPU is as follows:\n",
    "\n",
    "1. develop an application using generic array functionality, and test it on the CPU with the Array type\n",
    "2. port your application to the GPU by switching to the CuArray type\n",
    "3. disallow the CPU fallback (\"scalar indexing\") to find operations that are not implemented for or incompatible with GPU execution\n",
    "4. (optional) use lower-level, CUDA-specific interfaces to implement missing functionality or optimize performance"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Linear regression example"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "squared error loss function"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "loss(w, b, x, y) = sum(abs2, y - (w*x .+ b)) / size(y, 2)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "get gradient w.r.t to `w`"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "loss∇w(w, b, x, y) = ForwardDiff.gradient(w -> loss(w, b, x, y), w)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "get derivative w.r.t to `b` (`ForwardDiff.derivative` is\n",
    "used instead of `ForwardDiff.gradient` because `b` is\n",
    "a scalar instead of an array)"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "lossdb(w, b, x, y) = ForwardDiff.derivative(b -> loss(w, b, x, y), b)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "proximal gradient descent function"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function train(w, b, x, y; lr=0.1)\n",
    "    w -= lmul!(lr, loss∇w(w, b, x, y))\n",
    "    b -= lr * lossdb(w, b, x, y)\n",
    "    return w, b\n",
    "end\n",
    "\n",
    "n, p = 100, 10\n",
    "x = randn(n, p)'\n",
    "y = sum(x[1:5,:]; dims=1) .+ randn(n)' * 0.1\n",
    "w = 0.0001 * randn(1, p)\n",
    "b = 0.0\n",
    "\n",
    "err = Float64[]\n",
    "@showprogress 1 for i = 1:50\n",
    "   w, b = train(w, b, x, y)\n",
    "   push!(err, loss(w,b,x,y))\n",
    "end\n",
    "\n",
    "\n",
    "plot(err)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Moving to GPU"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "+"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "n, p = 100, 10\n",
    "x = randn(n, p)'\n",
    "y = sum(x[1:5,:]; dims=1) .+ randn(n)' * 0.1\n",
    "w = 0.0001 * randn(1, p)\n",
    "b = 0.0\n",
    "x = CuArray(x)\n",
    "y = CuArray(y)\n",
    "w = CuArray(w)\n",
    "\n",
    "err = Float64[]\n",
    "@showprogress 1 for i = 1:50\n",
    "   w, b = train(w, b, x, y)\n",
    "   push!(err, loss(w,b,x,y))\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "-"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "plot(err)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Custom Kernel"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- you cannot allocate memory,\n",
    "- I/O is disallowed,\n",
    "- badly-typed code will not compile.\n",
    "\n",
    "Keep kernels simple, and only incrementally port code while continuously verifying that it still compiles and executes as expected."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "+"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "a = CUDA.zeros(1024)\n",
    "\n",
    "function kernel(a)\n",
    "    i = threadIdx().x\n",
    "    a[i] += 1\n",
    "    return\n",
    "end\n",
    "\n",
    "@cuda threads=length(a) kernel(a)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "-"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "a = CUDA.rand(Int, 1000)\n",
    "\n",
    "norm(a)\n",
    "\n",
    "\n",
    "@btime norm($a)\n",
    "\n",
    "\n",
    "@btime norm($(Array(a)))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `norm` computation is much faster on the CPU"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "CUDA.allowscalar(false)\n",
    "\n",
    "\n",
    "a = CuArray(1:9_999_999);\n",
    "\n",
    "a .+ reverse(a);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "You need two kernels to perfom this last computation. @time is not the right tool to evaluate the elasped time. The task is scheduled on the GPU device but not executed. It will be executed when you will fetch the result on the CPU."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@time CUDA.@sync a .+ reverse(a);\n",
    "\n",
    "CUDA.@time a .+ reverse(a);\n",
    "\n",
    "@btime CUDA.@sync $a .+ reverse($a);\n",
    "\n",
    "@btime CUDA.@sync $(Array(a)) .+ reverse($(Array(a)));"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# NVIDIA Nsight Compute\n",
    "\n",
    "```bash\n",
    "$ nv-nsight-cu-cli --profile-from-start off julia\n",
    "```\n",
    "```julia-repl\n",
    "julia> CUDAdrv.@profile a .+ reverse(a)\n",
    "julia> exit()\n",
    "```\n",
    "\n",
    "```bash\n",
    "$ nsys launch julia\n",
    "```\n",
    "```julia-repl\n",
    "julia> CUDAdrv.@profile a .+ reverse(a)\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Interactive development"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "kernel() = (@cuprintln(\"foo\"); return)\n",
    "\n",
    "@cuda kernel()\n",
    "\n",
    "kernel() = (@cuprintln(\"foo\"); return)\n",
    "\n",
    "@cuda kernel()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "There is a significant overhead when you launch several kernels"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "a = CuArray(1:9_999_999)\n",
    "c = similar(a)\n",
    "c .= a .+ reverse(a);\n",
    "\n",
    "function vadd_reverse(c, a, b)\n",
    "    i = threadIdx().x\n",
    "    if i <= length(c)\n",
    "        @inbounds c[i] = a[i] + reverse(b)[i]\n",
    "    end\n",
    "    return\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Unsupported\n",
    "\n",
    "```julia\n",
    "@cuda threads = length(a) vadd_reverse(c, a, a)\n",
    "```\n",
    "will raise an error because some features are not supported on the GPU\n",
    "\n",
    "- Dynamic allocations\n",
    "- Garbage collection\n",
    "- Dynamic dispatch\n",
    "- Input/Output"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function vadd_reverse(c, a, b)\n",
    "\n",
    "    i = threadIdx().x\n",
    "    if i <= length(c)\n",
    "        @inbounds c[i] = a[i] + b[end - i + 1]\n",
    "    end\n",
    "    return\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following expression will also raise an error because you can't loop over an array\n",
    "on GPU in the same way.\n",
    "```julia\n",
    "@cuda threads = length(a) vadd_reverse(c, a, a)\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "attribute(device(), CUDA.DEVICE_ATTRIBUTE_MAX_THREADS_PER_BLOCK)\n",
    "\n",
    "function vadd_reverse(c, a, b)\n",
    "    i = (blockIdx().x - 1) * blockDim().x + threadIdx().x\n",
    "    if i <= length(c)\n",
    "        @inbounds c[i] = a[i] + b[end - i + 1]\n",
    "    end\n",
    "    return\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The kernel you built is twice faster"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@btime CUDA.@sync @cuda threads=1024 blocks=length($a)÷1024+1 vadd_reverse($c, $a, $a)\n",
    "\n",
    "@btime CUDA.@sync $a .+ reverse($a);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "To adapt the code to your device, use this configurator function"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function configurator(kernel)\n",
    "    config = launch_configuration(kernel.fun)\n",
    "    threads = min(length(a), config.threads)\n",
    "    blocks = cld(length(a), threads)\n",
    "    return (threads=threads, blocks=blocks)\n",
    "end\n",
    "\n",
    "@cuda config=configurator vadd_reverse(c, a, a)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Indexing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Cooperative groups\n",
    "\n",
    "https://devblogs.nvidia.com/cooperative-groups/\n",
    "\n",
    "```julia\n",
    "@cuda cooperative=true kernel(...)\n",
    "```\n",
    "\n",
    "# Shared memory\n",
    "\n",
    "https://devblogs.nvidia.com/using-shared-memory-cuda-cc/\n",
    "\n",
    "```julia\n",
    "a = @cuStaticSharedMem(Int, 64)\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dynamic parallelism\n",
    "\n",
    "https://devblogs.nvidia.com/cuda-dynamic-parallelism-api-principles/\n",
    "\n",
    "@cuda dynamic=true kernel(...)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Standard output\n",
    "\n",
    "```julia\n",
    "@cuprintln(\"Thread $(threadIdx().x)\")\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Atomics\n",
    "\n",
    "```julia\n",
    "@atomic a[...] += val\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## GPU programming performance tips\n",
    "\n",
    "- Avoid thread divergence (https://cvw.cac.cornell.edu/gpu/thread_div)\n",
    "- Reduce and coalesce global accesses\n",
    "- Improve occupancy\n",
    "- Early-free arrays  `CuArrays.unsafe_free!` (https://juliagpu.gitlab.io/CUDA.jl/usage/memory/)\n",
    "- Annotate with `@inbounds`\n",
    "- Use 32 bits for float and integers"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.2"
  },
  "kernelspec": {
   "name": "julia-1.5",
   "display_name": "Julia 1.5.2",
   "language": "julia"
  }
 },
 "nbformat": 4
}
